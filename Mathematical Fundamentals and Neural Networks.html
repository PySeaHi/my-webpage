<!DOCTYPE html><html lang="zh-CN"><head>
    <meta charset="UTF-8"/>
    <meta name="viewport" content="width=device-width, initial-scale=1.0"/>
    <title>概率分布与神经网络模型的深度关联</title>
    <script src="https://cdn.tailwindcss.com"></script>
    <script src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/3.2.2/es5/tex-mml-chtml.min.js"></script>
    <link href="https://fonts.googleapis.com/css2?family=Crimson+Text:ital,wght@0,400;0,600;1,400;1,600&amp;family=Inter:wght@300;400;500;600;700&amp;display=swap" rel="stylesheet"/>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/6.4.0/css/all.min.css"/>
    <style>
        :root {
            --primary: #1e293b;
            --secondary: #64748b;
            --accent: #d4a574;
            --bg-primary: #fefefe;
            --bg-secondary: #f8fafc;
            --text-primary: #0f172a;
            --text-secondary: #475569;
            --border: #e2e8f0;
        }
        
        body {
            font-family: 'Inter', -apple-system, BlinkMacSystemFont, sans-serif;
            color: var(--text-primary);
            background: var(--bg-primary);
            line-height: 1.7;
        }
        
        .serif {
            font-family: 'Crimson Text', Georgia, serif;
        }
        
        .hero-grid {
            display: grid;
            grid-template-columns: 2fr 1fr;
            gap: 2rem;
            align-items: center;
            min-height: 60vh;
        }
        
        .toc-fixed {
            position: fixed;
            top: 2rem;
            left: 2rem;
            width: 280px;
            max-height: calc(100vh - 4rem);
            overflow-y: auto;
            background: var(--bg-secondary);
            border: 1px solid var(--border);
            border-radius: 12px;
            padding: 1.5rem;
            z-index: 50;
            box-shadow: 0 10px 25px rgba(0,0,0,0.1);
        }
        
        .toc-fixed a {
            display: block;
            padding: 0.5rem 0;
            color: var(--text-secondary);
            text-decoration: none;
            border-left: 2px solid transparent;
            padding-left: 1rem;
            transition: all 0.2s ease;
        }
        
        .toc-fixed a:hover {
            color: var(--primary);
            border-left-color: var(--accent);
            background: rgba(212, 165, 116, 0.1);
        }
        
        .main-content {
            margin-left: 320px;
            padding: 2rem;
            max-width: calc(100vw - 360px);
        }
        
        .section-divider {
            height: 1px;
            background: linear-gradient(to right, transparent, var(--border), transparent);
            margin: 4rem 0;
        }
        
        .highlight-box {
            background: linear-gradient(135deg, #f8fafc 0%, #e2e8f0 100%);
            border-left: 4px solid var(--accent);
            padding: 1.5rem;
            margin: 2rem 0;
            border-radius: 0 8px 8px 0;
        }
        
        .formula-box {
            background: var(--bg-secondary);
            border: 1px solid var(--border);
            padding: 1.5rem;
            margin: 1.5rem 0;
            border-radius: 8px;
            position: relative;
            overflow: auto;
        }
        
        .formula-box:before {
            content: '';
            position: absolute;
            top: 0;
            left: 0;
            width: 4px;
            height: 100%;
            background: var(--accent);
            border-radius: 4px 0 0 4px;
        }
        
        .citation {
            display: inline-block;
            background: var(--accent);
            color: white;
            padding: 0.2rem 0.5rem;
            border-radius: 4px;
            font-size: 0.75rem;
            font-weight: 600;
            text-decoration: none;
            margin: 0 0.2rem;
            transition: all 0.2s ease;
        }
        
        .citation:hover {
            background: #b8935f;
            transform: translateY(-1px);
        }
        
        .hero-title {
            font-size: 3.5rem;
            font-weight: 600;
            line-height: 1.1;
            background: linear-gradient(135deg, var(--primary) 0%, var(--secondary) 100%);
            -webkit-background-clip: text;
            -webkit-text-fill-color: transparent;
            background-clip: text;
        }
        
        .hero-subtitle {
            font-size: 1.25rem;
            color: var(--text-secondary);
            font-style: italic;
            margin-top: 1rem;
        }
        
        .section-title {
            font-size: 2.5rem;
            font-weight: 600;
            color: var(--primary);
            margin: 3rem 0 1.5rem 0;
            border-bottom: 2px solid var(--accent);
            padding-bottom: 0.5rem;
        }
        
        .subsection-title {
            font-size: 1.75rem;
            font-weight: 600;
            color: var(--primary);
            margin: 2rem 0 1rem 0;
        }
        
        .chart-container {
            width: 100%;
            height: 400px;
            margin: 2rem 0;
            border: 1px solid var(--border);
            border-radius: 8px;
            padding: 1rem;
            background: white;
        }
        
        @media (max-width: 1024px) {
            .toc-fixed {
                display: none;
            }
            .main-content {
                margin-left: 0;
                max-width: 100%;
                padding: 1rem;
            }
            .hero-grid {
                grid-template-columns: 1fr;
                gap: 1rem;
            }
            .hero-title {
                font-size: 2.5rem;
            }
        }
    </style>
</head>
<body>
    <!-- Table of Contents -->
    <nav class="toc-fixed">
        <h3 class="text-lg font-semibold mb-4 text-slate-800">目录导航</h3>
        <div class="space-y-1 text-sm">
            <a href="#introduction">引言</a>
            <a href="#probability-overview">概率分布概述</a>
            <a href="#discrete-distributions">离散型概率分布</a>
            <a href="#continuous-distributions">连续型概率分布</a>
            <a href="#neural-network-applications">神经网络中的概率分布应用</a>
            <a href="#bernoulli-logistic">伯努利分布与逻辑回归</a>
            <a href="#categorical-softmax">范畴分布与多分类问题</a>
            <a href="#gaussian-regression">高斯分布与回归问题</a>
            <a href="#vae-distributions">变分自编码器中的分布</a>
            <a href="#gan-distributions">生成对抗网络中的分布</a>
            <a href="#bayesian-networks">贝叶斯神经网络</a>
            <a href="#reinforcement-learning">强化学习中的策略梯度</a>
        </div>
    </nav>

    <!-- Main Content -->
    <main class="main-content">
        <!-- Hero Section -->
        <section class="hero-grid mb-16">
            <div>
                <h1 class="hero-title serif">
                    <em>概率分布与神经网络模型</em>
                    <br/>的深度关联
                </h1>
                <p class="hero-subtitle">
                    探索随机性与智能计算的数学基础
                </p>
                <div class="mt-8 flex items-center space-x-4 text-sm text-slate-600">
                    <span><i class="fas fa-calendar-alt mr-2"></i>2025年6月</span>
                    <span><i class="fas fa-book mr-2"></i>结构化学习报告</span>
		    <span><i class="fas fa-user-edit mr-2"></i>Hale Sea</span>
                </div>
            </div>
            <div class="relative">
                <img src="https://kimi-web-img.moonshot.cn/img/ask.qcloudimg.com/6ed88d6360f4022164d89664d922401ea4f1f5bb.jpeg" alt="神经网络与概率分布融合的抽象艺术表现" class="w-full rounded-lg shadow-lg" size="large" aspect="wide" query="神经网络概率分布抽象艺术" referrerpolicy="no-referrer" data-modified="1" data-score="11598.00"/>
                <div class="absolute inset-0 bg-gradient-to-t from-slate-900/20 to-transparent rounded-lg"></div>
            </div>
        </section>

        <!-- Introduction -->
        <section id="introduction" class="mb-12">
            <h2 class="section-title serif">引言</h2>
            <div class="highlight-box">
                <p class="text-lg font-medium text-slate-800">
                    概率分布为神经网络提供了建模不确定性的数学基础，两者紧密相连。例如，逻辑回归使用伯努利分布处理二分类问题，Softmax回归利用范畴分布（多项式分布的特例）解决多分类问题，而回归任务常假设误差服从高斯分布。更复杂的模型如变分自编码器（VAE）和贝叶斯神经网络（BNN）则更深层次地融合了概率分布，前者用于学习数据的潜在表示，后者用于量化模型参数的不确定性。
                </p>
            </div>
        </section>

        <!-- Probability Distribution Overview -->
        <section id="probability-overview" class="mb-16">
            <h2 class="section-title serif">概率分布概述</h2>
            <p class="text-lg mb-6">
                概率分布是描述随机变量取值规律的数学工具，广泛应用于统计学、机器学习、信号处理等多个领域。根据随机变量的类型，概率分布主要分为离散型和连续型两大类。理解这些分布的特性、起源背景、数学表达及其在实际问题中的应用，对于深入掌握神经网络等现代机器学习模型至关重要。
            </p>
        </section>

        <div class="section-divider"></div>

        <!-- Discrete Distributions -->
        <section id="discrete-distributions" class="mb-16">
            <h2 class="section-title serif">离散型概率分布</h2>
            <p class="text-lg mb-8">
                离散型概率分布是统计学和概率论中研究离散随机变量行为的基础工具。这些分布描述了在有限或可数无限个可能结果中，每个结果发生的可能性。它们在各种领域都有广泛的应用，从简单的抛硬币实验到复杂的网络流量分析。
            </p>

            <!-- Bernoulli Distribution -->
            <div class="mb-12">
                <h3 id="bernoulli" class="subsection-title serif">伯努利分布（Bernoulli Distribution）</h3>
                <p class="mb-4">
                    <strong>伯努利分布</strong>是最简单的离散型概率分布之一，以瑞士数学家雅各布·伯努利（Jacob Bernoulli）的名字命名。它描述的是单次随机试验的结果，该试验只有两种可能的结果，通常称为&#34;成功&#34;和&#34;失败&#34;。例如，抛一枚硬币（正面或反面）、检查一个产品是否合格（合格或不合格）、或者一次射击是否命中目标（命中或不命中）等，都可以用伯努利试验来建模<a href="#ref1" class="citation">[1]</a>。
                </p>

                <div class="formula-box">
                    <h4 class="font-semibold mb-3">概率质量函数（PMF）</h4>
                    <div class="overflow-x-auto">
                        \[ P(X = x) = p^x (1 - p)^{1 - x} \]
                        <p class="text-sm text-slate-600 mt-2">其中，\(x\) 只能取0或1，\(p\) 为成功概率</p>
                    </div>
                </div>

                <div class="highlight-box">
                    <h4 class="font-semibold mb-2">神经网络应用</h4>
                    <p>
                        在<strong>逻辑回归模型</strong>中，逻辑回归常用于二分类问题，其输出可以被视为一个伯努利分布的参数 \(p\)。逻辑回归模型通过Sigmoid函数将线性组合 \(z = \theta^T X\) 映射到 \((0, 1)\) 区间，得到的值即为&#34;成功&#34;的概率 \(p = \sigma(z)\)<a href="#ref3" class="citation">[3]</a>。
                    </p>
                </div>
            </div>

            <!-- Binomial Distribution -->
            <div class="mb-12">
                <h3 id="binomial" class="subsection-title serif">二项分布（Binomial Distribution）</h3>
                <p class="mb-4">
                    <strong>二项分布</strong>是伯努利分布的推广，它描述了在 \(n\) 次独立的伯努利试验中，成功次数 \(k\) 的概率分布。每一次试验的成功概率均为 \(p\)，失败概率为 \(1-p\)。例如，抛 \(n\) 次硬币，统计正面朝上的次数；或者在批量生产的产品中，随机抽取 \(n\) 个产品，统计其中次品的数量<a href="#ref4" class="citation">[4]</a>。
                </p>

                <div class="formula-box">
                    <h4 class="font-semibold mb-3">概率质量函数（PMF）</h4>
                    <div class="overflow-x-auto">
                        \[ P(X = k) = \binom{n}{k} p^k (1 - p)^{n - k} \]
                        <p class="text-sm text-slate-600 mt-2">其中，\(\binom{n}{k} = \frac{n!}{k!(n-k)!}\) 是组合数</p>
                    </div>
                </div>
            </div>

            <!-- Multinomial Distribution -->
            <div class="mb-12">
                <h3 id="multinomial" class="subsection-title serif">多项式分布（Multinomial Distribution）</h3>
                <p class="mb-4">
                    <strong>多项式分布</strong>是二项分布的进一步推广，它描述的是进行 \(n\) 次独立试验，每次试验有 \(m\) (\(m \ge 2\)) 种可能的结果，并且每种结果发生的概率分别为 \(p_1, p_2, \ldots, p_m\)。
                </p>

                <div class="formula-box">
                    <h4 class="font-semibold mb-3">概率质量函数（PMF）</h4>
                    <div class="overflow-x-auto">
                        \[ P(X_1 = k_1, \ldots, X_m = k_m) = \frac{n!}{k_1! \ldots k_m!} p_1^{k_1} \ldots p_m^{k_m} \]
                    </div>
                </div>

                <div class="highlight-box">
                    <h4 class="font-semibold mb-2">神经网络应用</h4>
                    <p>
                        在<strong>神经网络的多分类任务中，Softmax函数的输出可以被视为多项式分布的参数 \(p_i\)，表示样本属于各个类别的概率</strong><a href="#ref9" class="citation">[9]</a>。
                    </p>
                </div>
            </div>
        </section>

        <div class="section-divider"></div>

        <!-- Continuous Distributions -->
        <section id="continuous-distributions" class="mb-16">
            <h2 class="section-title serif">连续型概率分布</h2>
            <p class="text-lg mb-8">
                连续型概率分布描述的是随机变量在连续区间内取值的概率规律。与离散型随机变量不同，连续型随机变量可以取无限多个值，因此我们通常使用概率密度函数（PDF）来描述其分布。
            </p>

            <!-- Uniform Distribution -->
            <div class="mb-12">
                <h3 id="uniform" class="subsection-title serif">均匀分布（Uniform Distribution）</h3>
                <p class="mb-4">
                    <strong>均匀分布</strong>是最简单的连续型概率分布之一，它描述的是随机变量在某个有限区间 \([a, b]\) 内取值的概率是均等的。例如，一个理想的随机数生成器在 \([0, 1]\) 区间生成的数字服从均匀分布。
                </p>

                <div class="formula-box">
                    <h4 class="font-semibold mb-3">概率密度函数（PDF）</h4>
                    <div class="overflow-x-auto">
                        \[ f(x) = \begin{cases} \frac{1}{b - a} &amp; \text{若 } a \le x \le b \\ 0 &amp; \text{其他情况} \end{cases} \]
                    </div>
                </div>
            </div>

            <!-- Normal Distribution -->
            <div class="mb-12">
                <h3 id="normal" class="subsection-title serif">正态分布（Normal Distribution）</h3>
                <p class="mb-4">
                    <strong>正态分布</strong>，也称为<strong>高斯分布</strong>，是统计学和概率论中最重要的连续型概率分布之一。它由德国数学家卡尔·弗里德里希·高斯在研究测量误差时提出。正态分布的重要性在于<strong>中心极限定理</strong>，该定理指出，大量独立同分布的随机变量之和的分布会趋近于正态分布。
                </p>

                <div class="formula-box">
                    <h4 class="font-semibold mb-3">概率密度函数（PDF）</h4>
                    <div class="overflow-x-auto">
                        \[ f(x) = \frac{1}{\sigma \sqrt{2\pi}} e^{-\frac{(x - \mu)^2}{2\sigma^2}} \]
                    </div>
                </div>

                <div class="highlight-box">
                    <h4 class="font-semibold mb-2">神经网络应用</h4>
                    <p>
                        在<strong>变分自编码器（VAE）</strong>中，潜在变量通常被假设服从标准正态分布，而重构损失可以看作是在高斯分布假设下的负对数似然<a href="#ref12" class="citation">[12]</a><a href="#ref13" class="citation">[13]</a>。神经网络的权重初始化也常采用正态分布，例如 He 初始化或 Kaiming 初始化<a href="#ref11" class="citation">[11]</a><a href="#ref14" class="citation">[14]</a>。
                    </p>
                </div>
            </div>
        </section>

        <div class="section-divider"></div>

        <!-- Neural Network Applications -->
        <section id="neural-network-applications" class="mb-16">
            <h2 class="section-title serif">神经网络中的概率分布应用</h2>
            <p class="text-lg mb-8">
                神经网络模型与概率分布之间存在着深刻而广泛的联系。许多经典的神经网络结构和训练方法都可以从概率的角度进行理解和解释。通过将神经网络的输出层与特定的概率分布联系起来，并选择相应的损失函数（通常是该分布的负对数似然），我们可以构建出能够进行概率预测和不确定性估计的模型。
            </p>
        </section>

        <!-- Bernoulli and Logistic Regression -->
        <section id="bernoulli-logistic" class="mb-16">
            <h2 class="section-title serif">伯努利分布与逻辑回归/二分类问题</h2>
            <p class="mb-6">
                <strong>伯努利分布</strong>是描述单次二元随机试验结果的基础分布。在神经网络中，<strong>逻辑回归（Logistic Regression）</strong>是最直接应用伯努利分布的二分类模型。逻辑回归假设给定输入特征 \( \mathbf{x} \)，输出变量 \( y \) 服从参数为 \( p \) 的伯努利分布。
            </p>

            <div class="formula-box">
                <h4 class="font-semibold mb-3">Sigmoid 激活函数</h4>
                <div class="overflow-x-auto">
                    \[ \sigma(z) = \frac{1}{1 + e^{-z}} \]
                    <p class="text-sm text-slate-600 mt-2">将线性组合 \(z = \mathbf{w}^T\mathbf{x} + b\) 映射到 (0, 1) 区间</p>
                </div>
            </div>

            <div class="formula-box">
                <h4 class="font-semibold mb-3">二元交叉熵损失函数</h4>
                <div class="overflow-x-auto">
                    \[ \mathcal{L}(\mathbf{w}, b) = -\sum_{i=1}^N \left[ y_i \log(\sigma(\mathbf{w}^T\mathbf{x}_i + b)) + (1-y_i) \log(1 - \sigma(\mathbf{w}^T\mathbf{x}_i + b)) \right] \]
                </div>
            </div>

            <div class="highlight-box">
                <h4 class="font-semibold mb-2">应用实例：垃圾邮件检测</h4>
                <p>
                    在垃圾邮件检测中，输入 \( \mathbf{x} \) 可以是邮件的特征向量，输出 \( y=1 \) 表示邮件是垃圾邮件，\( y=0 \) 表示非垃圾邮件。神经网络通过学习参数，使得对于垃圾邮件，\( \sigma(\mathbf{w}^T\mathbf{x} + b) \) 接近1，对于非垃圾邮件则接近0<a href="#ref3" class="citation">[3]</a><a href="#ref18" class="citation">[18]</a>。
                </p>
            </div>
        </section>

        <!-- Categorical and Softmax -->
        <section id="categorical-softmax" class="mb-16">
            <h2 class="section-title serif">范畴分布（多项式分布）与多分类问题</h2>
            <p class="mb-6">
                当分类问题扩展到 \( K \) (\( K &gt; 2 \)) 个互斥的类别时，需要使用<strong>范畴分布</strong>（多项式分布在单次试验时的特例）。在神经网络的多分类任务中，输出层通常采用 <strong>Softmax 激活函数</strong>。
            </p>

            <div class="formula-box">
                <h4 class="font-semibold mb-3">Softmax 函数</h4>
                <div class="overflow-x-auto">
                    \[ \text{Softmax}(z_k) = p_k = \frac{e^{z_k}}{\sum_{j=1}^K e^{z_j}} \]
                    <p class="text-sm text-slate-600 mt-2">将 \( K \) 维实数向量映射为概率向量</p>
                </div>
            </div>

            <div class="formula-box">
                <h4 class="font-semibold mb-3">交叉熵损失函数</h4>
                <div class="overflow-x-auto">
                    \[ \mathcal{L}(\theta) = -\sum_{i=1}^N \sum_{k=1}^K y_{ik} \log(\text{Softmax}(f_k(\mathbf{x}_i; \theta))) \]
                </div>
            </div>

            <div class="highlight-box">
                <h4 class="font-semibold mb-2">应用实例：MNIST手写数字识别</h4>
                <p>
                    在 MNIST 手写数字识别任务中，输入是一张灰度图像，输出是一个10维的概率向量，表示该图像属于数字0到9的概率。通过最小化交叉熵损失，网络学习到的参数能够使得正确类别的预测概率尽可能高<a href="#ref9" class="citation">[9]</a><a href="#ref20" class="citation">[20]</a>。
                </p>
            </div>
        </section>

        <!-- Gaussian and Regression -->
        <section id="gaussian-regression" class="mb-16">
            <h2 class="section-title serif">高斯分布与回归问题/深度学习中的潜在应用</h2>
            <p class="mb-6">
                <strong>高斯分布</strong>在神经网络中，尤其是在回归问题和一些生成模型中，扮演着重要角色。在标准的回归问题中，目标是预测一个连续值，通常假设输出服从高斯分布。
            </p>

            <div class="formula-box">
                <h4 class="font-semibold mb-3">高斯分布假设</h4>
                <div class="overflow-x-auto">
                    \[ y \sim \mathcal{N}(\mu(\mathbf{x}), \sigma^2(\mathbf{x})) \]
                    <p class="text-sm text-slate-600 mt-2">神经网络学习均值函数 \(\mu(\mathbf{x}; \theta)\) 和方差函数 \(\sigma^2(\mathbf{x}; \theta)\)</p>
                </div>
            </div>

            <div class="formula-box">
                <h4 class="font-semibold mb-3">异方差回归损失函数</h4>
                <div class="overflow-x-auto">
                    \[ \mathcal{L}(\theta) = \frac{1}{2} \log(\sigma^2(\mathbf{x}; \theta)) + \frac{(y - \mu(\mathbf{x}; \theta))^2}{2\sigma^2(\mathbf{x}; \theta)} \]
                </div>
            </div>
        </section>

        <div class="section-divider"></div>

        <!-- VAE Distributions -->
        <section id="vae-distributions" class="mb-16">
            <h2 class="section-title serif">变分自编码器 (VAE) 中的概率分布</h2>
            <p class="mb-6">
                <strong>变分自编码器 (VAE)</strong> 是一种强大的生成模型，其核心思想在于学习数据在潜在空间中的概率分布，并通过从这个分布中采样来生成新的数据样本<a href="#ref23" class="citation">[23]</a><a href="#ref12" class="citation">[12]</a>。
            </p>

            <div class="formula-box">
                <h4 class="font-semibold mb-3">证据下界 (ELBO)</h4>
                <div class="overflow-x-auto">
                    \[ \text{ELBO} = \mathbb{E}_{z \sim q_\phi(z|x)}[\log p_\theta(x|z)] - D_{KL}(q_\phi(z|x) || p(z)) \]
                    <p class="text-sm text-slate-600 mt-2">由重构损失和KL散度组成</p>
                </div>
            </div>

            <div class="formula-box">
                <h4 class="font-semibold mb-3">KL散度（对角协方差高斯分布）</h4>
                <div class="overflow-x-auto">
                    \[ D_{KL}(q_\phi(\mathbf{z}|\mathbf{x}) \| p(\mathbf{z})) = \frac{1}{2} \sum_{j=1}^J (1 + \log(\sigma_j^2) - \mu_j^2 - \sigma_j^2) \]
                </div>
            </div>

            <div class="highlight-box">
                <h4 class="font-semibold mb-2">重参数化技巧</h4>
                <p>
                    通过重参数化技巧从编码器输出的分布中采样：\(\mathbf{z} = \boldsymbol{\mu}_\mathbf{z} + \boldsymbol{\sigma}_\mathbf{z} \odot \boldsymbol{\epsilon}, \quad \boldsymbol{\epsilon} \sim \mathcal{N}(0, \mathbf{I})\)，这保证了梯度可导，使得模型可以通过反向传播进行训练<a href="#ref13" class="citation">[13]</a>。
                </p>
            </div>
        </section>

        <!-- GAN Distributions -->
        <section id="gan-distributions" class="mb-16">
            <h2 class="section-title serif">生成对抗网络 (GAN) 中的概率分布</h2>
            <p class="mb-6">
                <strong>生成对抗网络 (GANs)</strong> 通过一个对抗过程来学习数据的潜在分布。生成器从简单先验分布（通常是标准正态分布或均匀分布）中采样噪声向量，试图生成与真实数据相似的样本。
            </p>

            <div class="formula-box">
                <h4 class="font-semibold mb-3">GAN价值函数</h4>
                <div class="overflow-x-auto">
                    \[ \min_G \max_D V(D, G) = \mathbb{E}_{x \sim p_{data}(x)}[\log D(x)] + \mathbb{E}_{z \sim p_z(z)}[\log (1 - D(G(z)))] \]
                </div>
            </div>

            <div class="highlight-box">
                <h4 class="font-semibold mb-2">分布隐式学习</h4>
                <p>
                    虽然GAN不直接对数据分布进行显式参数化建模，但其训练过程隐式地驱使生成分布 \(p_g(x)\) 逼近真实数据分布 \(p_{data}(x)\)。当达到纳什均衡时，理想情况下 \(p_g(x) = p_{data}(x)\)。
                </p>
            </div>
        </section>

        <!-- Bayesian Neural Networks -->
        <section id="bayesian-networks" class="mb-16">
            <h2 class="section-title serif">贝叶斯神经网络中的概率分布</h2>
            <p class="mb-6">
                <strong>贝叶斯神经网络 (BNNs)</strong> 将网络中的权重和偏置等参数 \(\mathbf{w}\) 视为随机变量，而不是固定的确定值<a href="#ref24" class="citation">[24]</a><a href="#ref25" class="citation">[25]</a>。这意味着BNN为每个参数引入了一个概率分布，通常是先验分布 \(p(\mathbf{w})\)。
            </p>

            <div class="formula-box">
                <h4 class="font-semibold mb-3">变分推断证据下界</h4>
                <div class="overflow-x-auto">
                    \[ \text{ELBO}(\theta) = \mathbb{E}_{\mathbf{w} \sim q_\theta(\mathbf{w})}[\log p(D|\mathbf{w})] - D_{KL}(q_\theta(\mathbf{w}) || p(\mathbf{w})) \]
                </div>
            </div>

            <div class="highlight-box">
                <h4 class="font-semibold mb-2">不确定性量化</h4>
                <p>
                    BNN的关键优势是能够量化预测的不确定性，包括认知不确定性（模型参数的不确定性）和偶然不确定性（数据本身的噪声）。例如，使用高斯分布作为权重的变分后验，其方差表示了参数的不确定性<a href="#ref24" class="citation">[24]</a><a href="#ref26" class="citation">[26]</a>。
                </p>
            </div>
        </section>

        <!-- Reinforcement Learning -->
        <section id="reinforcement-learning" class="mb-16">
            <h2 class="section-title serif">强化学习中的策略梯度与概率分布</h2>
            <p class="mb-6">
                在<strong>强化学习</strong>的策略梯度算法中，概率分布扮演着核心角色。智能体的策略通常定义为一个概率分布 \(\pi(a|s)\)，表示在状态 \(s\) 下选择动作 \(a\) 的概率。
            </p>

            <div class="formula-box">
                <h4 class="font-semibold mb-3">策略梯度定理</h4>
                <div class="overflow-x-auto">
                    \[ \nabla_\theta J(\theta) \propto \mathbb{E}_{\pi_\theta} \left[ G_t \nabla_\theta \log \pi_\theta(A_t | S_t) \right] \]
                </div>
            </div>

            <div class="highlight-box">
                <h4 class="font-semibold mb-2">动作分布建模</h4>
                <p>
                    在离散动作空间中，策略网络输出Softmax概率向量（范畴分布）；在连续动作空间中，策略网络可以输出高斯分布的均值和方差。这种将神经网络输出解释为动作概率分布的方法，是许多现代强化学习算法的基础。
                </p>
            </div>
        </section>

        <!-- References -->
        <section class="mb-16">
            <h2 class="section-title serif">参考文献</h2>
            <div class="space-y-4 text-sm">
                <div id="ref1" class="border-l-4 border-amber-400 pl-4">
                    <strong>[1]</strong> 伯努利分布基础. <a href="https://www.cnblogs.com/FavoriteStar/p/16985482.html" class="text-blue-600 hover:underline">https://www.cnblogs.com/FavoriteStar/p/16985482.html</a>
                </div>
                <div id="ref2" class="border-l-4 border-amber-400 pl-4">
                    <strong>[2]</strong> 概率分布期望与方差. <a href="https://zhuanlan.zhihu.com/p/60976262" class="text-blue-600 hover:underline">https://zhuanlan.zhihu.com/p/60976262</a>
                </div>
                <div id="ref3" class="border-l-4 border-amber-400 pl-4">
                    <strong>[3]</strong> 逻辑回归与伯努利分布. <a href="https://blog.csdn.net/zhm2229/article/details/100970777" class="text-blue-600 hover:underline">https://blog.csdn.net/zhm2229/article/details/100970777</a>
                </div>
                <div id="ref4" class="border-l-4 border-amber-400 pl-4">
                    <strong>[4]</strong> 二项分布应用. <a href="https://blog.csdn.net/lekusun9671/article/details/82349113" class="text-blue-600 hover:underline">https://blog.csdn.net/lekusun9671/article/details/82349113</a>
                </div>
                <div id="ref5" class="border-l-4 border-amber-400 pl-4">
                    <strong>[5]</strong> 概率分布详解. <a href="https://zhuanlan.zhihu.com/p/589283579" class="text-blue-600 hover:underline">https://zhuanlan.zhihu.com/p/589283579</a>
                </div>
                <div id="ref6" class="border-l-4 border-amber-400 pl-4">
                    <strong>[6]</strong> 几何分布应用. <a href="https://www.ucloud.cn/yun/41184.html" class="text-blue-600 hover:underline">https://www.ucloud.cn/yun/41184.html</a>
                </div>
                <div id="ref7" class="border-l-4 border-amber-400 pl-4">
                    <strong>[7]</strong> Softmax函数详解. <a href="https://tsinghua-gongjing.github.io/posts/softmax-function.html" class="text-blue-600 hover:underline">https://tsinghua-gongjing.github.io/posts/softmax-function.html</a>
                </div>
                <div id="ref8" class="border-l-4 border-amber-400 pl-4">
                    <strong>[8]</strong> 泊松分布近似. <a href="https://www.cnblogs.com/yinheyi/p/6131262.html" class="text-blue-600 hover:underline">https://www.cnblogs.com/yinheyi/p/6131262.html</a>
                </div>
                <div id="ref9" class="border-l-4 border-amber-400 pl-4">
                    <strong>[9]</strong> Softmax与多项式分布. <a href="https://www.heywhale.com/mw/notebook/6436195faf944ada5ce28ea5" class="text-blue-600 hover:underline">https://www.heywhale.com/mw/notebook/6436195faf944ada5ce28ea5</a>
                </div>
                <div id="ref10" class="border-l-4 border-amber-400 pl-4">
                    <strong>[10]</strong> 变分自编码器基础. <a href="https://www.ibm.com/cn-zh/think/topics/variational-autoencoder" class="text-blue-600 hover:underline">https://www.ibm.com/cn-zh/think/topics/variational-autoencoder</a>
                </div>
                <div id="ref11" class="border-l-4 border-amber-400 pl-4">
                    <strong>[11]</strong> 神经网络初始化. <a href="https://www.cnblogs.com/GraphL/p/18707921" class="text-blue-600 hover:underline">https://www.cnblogs.com/GraphL/p/18707921</a>
                </div>
                <div id="ref12" class="border-l-4 border-amber-400 pl-4">
                    <strong>[12]</strong> VAE中的高斯分布. <a href="https://zhuanlan.zhihu.com/p/678864844" class="text-blue-600 hover:underline">https://zhuanlan.zhihu.com/p/678864844</a>
                </div>
                <div id="ref13" class="border-l-4 border-amber-400 pl-4">
                    <strong>[13]</strong> VAE技术详解. <a href="https://www.vectorexplore.com/tech/auto-encoder/vae.html" class="text-blue-600 hover:underline">https://www.vectorexplore.com/tech/auto-encoder/vae.html</a>
                </div>
                <div id="ref14" class="border-l-4 border-amber-400 pl-4">
                    <strong>[14]</strong> 权重初始化方法. <a href="https://blog.csdn.net/benny_zhou2004/article/details/140448583" class="text-blue-600 hover:underline">https://blog.csdn.net/benny_zhou2004/article/details/140448583</a>
                </div>
                <div id="ref15" class="border-l-4 border-amber-400 pl-4">
                    <strong>[15]</strong> 伽马与贝塔函数. <a href="https://blog.csdn.net/benny_zhou2004/article/details/140448583" class="text-blue-600 hover:underline">https://blog.csdn.net/benny_zhou2004/article/details/140448583</a>
                </div>
                <div id="ref16" class="border-l-4 border-amber-400 pl-4">
                    <strong>[16]</strong> 证据深度学习. <a href="https://blog.csdn.net/qq_36033058/article/details/107454257" class="text-blue-600 hover:underline">https://blog.csdn.net/qq_36033058/article/details/107454257</a>
                </div>
                <div id="ref17" class="border-l-4 border-amber-400 pl-4">
                    <strong>[17]</strong> 统计学笔记. <a href="https://hep.tsinghua.edu.cn/~orv/teaching/statistics/note.pdf" class="text-blue-600 hover:underline">https://hep.tsinghua.edu.cn/~orv/teaching/statistics/note.pdf</a>
                </div>
                <div id="ref18" class="border-l-4 border-amber-400 pl-4">
                    <strong>[18]</strong> 概率分布总结. <a href="https://www.biaodianfu.com/distributions.html" class="text-blue-600 hover:underline">https://www.biaodianfu.com/distributions.html</a>
                </div>
                <div id="ref19" class="border-l-4 border-amber-400 pl-4">
                    <strong>[19]</strong> Softmax函数详解. <a href="https://cloud.tencent.com/developer/article/2153323" class="text-blue-600 hover:underline">https://cloud.tencent.com/developer/article/2153323</a>
                </div>
                <div id="ref20" class="border-l-4 border-amber-400 pl-4">
                    <strong>[20]</strong> 分类损失函数专利. <a href="https://patents.google.com/patent/CN113344174A/zh" class="text-blue-600 hover:underline">https://patents.google.com/patent/CN113344174A/zh</a>
                </div>
                <div id="ref21" class="border-l-4 border-amber-400 pl-4">
                    <strong>[21]</strong> 神经主题模型. <a href="https://www.cnblogs.com/net/p/18717323" class="text-blue-600 hover:underline">https://www.cnblogs.com/net/p/18717323</a>
                </div>
                <div id="ref22" class="border-l-4 border-amber-400 pl-4">
                    <strong>[22]</strong> 主题模型实现. <a href="https://blog.csdn.net/lsb2002/article/details/134937677" class="text-blue-600 hover:underline">https://blog.csdn.net/lsb2002/article/details/134937677</a>
                </div>
                <div id="ref23" class="border-l-4 border-amber-400 pl-4">
                    <strong>[23]</strong> VAE原理解析. <a href="https://yuanzhuo.bnu.edu.cn/article/749" class="text-blue-600 hover:underline">https://yuanzhuo.bnu.edu.cn/article/749</a>
                </div>
                <div id="ref24" class="border-l-4 border-amber-400 pl-4">
                    <strong>[24]</strong> 贝叶斯神经网络. <a href="https://time.geekbang.org/column/article/13213" class="text-blue-600 hover:underline">https://time.geekbang.org/column/article/13213</a>
                </div>
                <div id="ref25" class="border-l-4 border-amber-400 pl-4">
                    <strong>[25]</strong> BNN应用研究. <a href="https://isbreedingen.caas.cn/pub/zwkxyjs/docs/20210924124203231270.pdf" class="text-blue-600 hover:underline">https://isbreedingen.caas.cn/pub/zwkxyjs/docs/20210924124203231270.pdf</a>
                </div>
                <div id="ref26" class="border-l-4 border-amber-400 pl-4">
                    <strong>[26]</strong> 变分推断方法. <a href="https://www.woshipm.com/data-analysis/5391963.html" class="text-blue-600 hover:underline">https://www.woshipm.com/data-analysis/5391963.html</a>
                </div>
            </div>
        </section>
    </main>

    <script>
        // Smooth scrolling for anchor links
        document.querySelectorAll('a[href^="#"]').forEach(anchor => {
            anchor.addEventListener('click', function (e) {
                e.preventDefault();
                const target = document.querySelector(this.getAttribute('href'));
                if (target) {
                    target.scrollIntoView({
                        behavior: 'smooth',
                        block: 'start'
                    });
                }
            });
        });

        // Highlight active section in TOC
        const sections = document.querySelectorAll('section[id]');
        const tocLinks = document.querySelectorAll('.toc-fixed a');

        function highlightActiveSection() {
            let current = '';
            sections.forEach(section => {
                const sectionTop = section.offsetTop;
                if (window.scrollY >= sectionTop - 100) {
                    current = section.getAttribute('id');
                }
            });

            tocLinks.forEach(link => {
                link.classList.remove('text-slate-900', 'font-semibold', 'border-amber-400');
                if (link.getAttribute('href') === '#' + current) {
                    link.classList.add('text-slate-900', 'font-semibold', 'border-amber-400');
                }
            });
        }

        window.addEventListener('scroll', highlightActiveSection);
        highlightActiveSection(); // Initial call
    </script>

</body></html>